extraPythonPackages:
  - jupyterhub-unicorespawner==0.9.1
extraVolumes:
  - configMap:
      defaultMode: 256
      name: unicore-jobs-common
    name: job-descriptions-common
  - configMap:
      defaultMode: 256
      name: unicore-jobs-3-6
    name: job-descriptions-3-6
  - configMap:
      defaultMode: 256
      name: unicore-jobs-3-4
    name: job-descriptions-3-4
  - configMap:
      defaultMode: 256
      name: unicore-jobs-3-3
    name: job-descriptions-3-3
  - configMap:
      defaultMode: 256
      name: unicore-jobs-3-2
    name: job-descriptions-3-2
  - configMap:
      defaultMode: 256
      name: unicore-jobs-2-6
    name: job-descriptions-2-6
extraVolumeMounts:
  - mountPath: /mnt/jobs/common
    name: job-descriptions-common
  - mountPath: /mnt/jobs/JupyterLab_3.6
    name: job-descriptions-3-6
  - mountPath: /mnt/jobs/JupyterLab_3.4
    name: job-descriptions-3-4
  - mountPath: /mnt/jobs/JupyterLab_3.3
    name: job-descriptions-3-3
  - mountPath: /mnt/jobs/JupyterLab_3.2
    name: job-descriptions-3-2
  - mountPath: /mnt/jobs/JupyterLab_2.6
    name: job-descriptions-2-6
outpostConfig: |-
  import json
  import logging
  import pyunicore
  import os

  from unicorespawner import UnicoreSpawner

  class HealthCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          return record.getMessage().find("/ping") == -1

  # Remove /credentials/health from application server logs
  logging.getLogger("uvicorn.access").addFilter(HealthCheckFilter())

  logged_logger_name = os.environ.get("LOGGER_NAME", "OutpostUnicore")
  c.JupyterHubOutpost.log_format = f"%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d {logged_logger_name} %(name)s %(module)s:%(lineno)d]%(end_color)s %(message)s"

  async def sanitize_start_response(spawner, original_response):
    spawner.log.info(f"{spawner._log_name} - Replace original response ({original_response}) with ''")
    return ""

  c.JupyterHubOutpost.sanitize_start_response = sanitize_start_response

  c.JupyterHubOutpost.send_events = False

  c.JupyterHubOutpost.spawner_class = UnicoreSpawner

  async def job_description(spawner, user_options):
    jhub = spawner.jupyterhub_name
    spawner.log.info(f"{spawner._log_name} - Spawn for {jhub} with user_options: {user_options}")
    profile = user_options["profile"].replace("/", "_")
    system = user_options["system"]
    account = user_options["account"]
    project = user_options["project"]
    partition = user_options["partition"]
    
    # Check if system is configured
    if system not in system_config.keys():
      raise Exception(f"{spwaner._log_name} - system {system} currently not configured")
    
    jhub_job_description = f"/mnt/jobs/{profile}/{jhub}_job_description.json"
    version_job_description = f"/mnt/jobs/{profile}/job_description.json"
    common_job_description = f"/mnt/jobs/common/job_description.json"

    if os.path.isfile(jhub_job_description):
      with open(jhub_job_description, "r") as f:
        job_description = json.load(f)
    elif os.path.isfile(version_job_description):
      with open(version_job_description, "r") as f:
        job_description = json.load(f)
    elif os.path.isfile(common_job_description):
      with open(common_job_description, "r") as f:
        job_description = json.load(f)
    else:
      raise Exception(f"{spawner._log_name} - Could not find job_description for job {profile}")
    
    job_description["Imports"] = []
    job_description["Environment"] = spawner.get_env()
    
    if spawner.internal_ssl:
      # First add Import files for certificates.
      # Allows to override them later, if necessary
      with open(spawner.cert_paths["keyfile"], "r") as f:
        job_description["Imports"].append({
          "From": "inline://dummy",
          "To": "service_cert.key",
          "Data": f.read()
        })
      with open(spawner.cert_paths["certfile"], "r") as f:
        job_description["Imports"].append({
          "From": "inline://dummy",
          "To": "service_cert.crt",
          "Data": f.read()
        })
      with open(spawner.cert_paths["cafile"], "r") as f:
        cafile = f.read()
      with open(spawner.internal_trust_bundles['hub-ca'], "r") as f:
        hubca = f.read()
      job_description["Imports"].append({
        "From": "inline://dummy",
        "To": "service_ca.crt",
        "Data": cafile + hubca
      })
      job_description["Environment"]["JUPYTERHUB_SSL_CERTFILE"] = "service_cert.crt"
      job_description["Environment"]["JUPYTERHUB_SSL_KEYFILE"] = "service_cert.key"
      job_description["Environment"]["JUPYTERHUB_SSL_CLIENT_CA"] = "service_ca.crt"
    
    prefix = "input_"
    jhub_prefix = f"{jhub}_input_"
    system_prefix = f"{system}_input_"
    jhub_system_prefix = f"{jhub}_{system}_input_"
    # First collect common input files
    for subdir, dirs, files in os.walk("/mnt/jobs/common"):
      for file in files:
        if file.startswith(prefix):
          with open(os.path.join(subdir, file), "r") as f:
            job_description["Imports"].append({
              "From": "inline://dummy",
              "To": file[len(prefix):],
              "Data": f.read()
            })

    # Afterwards collect job specific files. Allows easy override of common files
    spawner.log.info(f"{spawner._log_name} - Walk through: /mnt/jobs/{profile}")
    for subdir, dirs, files in os.walk(f"/mnt/jobs/{profile}"):
      for file in files:
        spawner.log.info(f"{spawner._log_name} - profile specific file: {file}")
        if file.startswith(prefix):
          spawner.log.info(f"{spawner._log_name} - Add as filename: {file[len(prefix):]}")
          with open(os.path.join(subdir, file), "r") as f:
            job_description["Imports"].append({
              "From": "inline://dummy",
              "To": file[len(prefix):],
              "Data": f.read()
            })
    
    # Afterwards collect jhub specific files. Allows easy override of previous files
    for subdir, dirs, files in os.walk(f"/mnt/jobs/{profile}"):
      for file in files:
        if file.startswith(jhub_prefix):
          with open(os.path.join(subdir, file), "r") as f:
            job_description["Imports"].append({
              "From": "inline://dummy",
              "To": file[len(jhub_prefix):],
              "Data": f.read()
            })
    
    # Afterwards collect system specific files. Allows easy override of previous files
    for subdir, dirs, files in os.walk(f"/mnt/jobs/{profile}"):
      for file in files:
        if file.startswith(system_prefix):
          with open(os.path.join(subdir, file), "r") as f:
            job_description["Imports"].append({
              "From": "inline://dummy",
              "To": file[len(system_prefix):],
              "Data": f.read()
            })
    
    # Afterwards collect jhub-system specific files. Allows easy override of previous files
    for subdir, dirs, files in os.walk(f"/mnt/jobs/{profile}"):
      for file in files:
        if file.startswith(jhub_system_prefix):
          with open(os.path.join(subdir, file), "r") as f:
            job_description["Imports"].append({
              "From": "inline://dummy",
              "To": file[len(jhub_system_prefix):],
              "Data": f.read()
            })

    if partition in system_config[system]["interactive_partitions"]:
      job_description["Job type"] = "on_login_node"
      job_description["Login node"] = system_config[system]["interactive_partitions"][partition]

      # if internal_ssl is usedon login node, we don't need the ca variable. 
      # We're using the public_api_url, not the internal one
      if "JUPYTERHUB_SSL_CLIENT_CA" in job_description["Environment"].keys():
        del job_description["Environment"]["JUPYTERHUB_SSL_CLIENT_CA"]    
    else:
      nodes = user_options["nodes"]
      runtime = user_options["runtime"]
      gpus = user_options.get("gpus", False)
      xserver = user_options.get("xserver", False)
      reservation = user_options.get("reservation", False)

      job_description["Job type"] = "batch"
      job_description["Resources"] = {
        "Queue": partition,
        "Nodes": nodes,
        "Runtime": f"{runtime}min"
      }
      if gpus:
        job_description["Resources"]["GPUS"] = gpus
      if xserver:
        job_description["Resources"]["StartXServer"] = xserver
      if reservation: 
        job_description["Resources"]["Reservation"] = reservation
    
    
    jd_to_log = job_description.copy()
    if "Imports" in jd_to_log.keys():
      jd_to_log["Imports"] = "..."
    if "Environment" in jd_to_log.keys():
      jd_to_log["Environment"] = "..."
    spawner.log.info(f"{spawner._log_name} - Use job_description: {jd_to_log}")
    spawner.log.debug(f"{spawner._log_name} - Start job with these input files: {[x.get('To') for x in job_description.get('Imports', [])]}")
    spawner.log.debug(f"{spawner._log_name} - Start job with these environment variables: {list(job_description.get('Environment').keys())}")

    return job_description


  additional_replacements_base = {
    "system": {
      "JUWELS": {
        "hostname_base": "jwlogin",
        "hostname_cpulimit": "login|vis",
        "hostname_all": "'jwlogin02i' 'jwlogin03i' 'jwlogin04i' 'jwlogin05i' 'jwlogin06i' 'jwlogin07i' 'jwlogin08i' 'jwlogin09i' 'jwlogin10i' 'jwlogin11i'"
      },
      "JURECA": {
        "hostname_base": "jrlogin",
        "hostname_cpulimit": "login|vis",
        "hostname_all": "'jrlogin01i' 'jrlogin02i' 'jrlogin03i' 'jrlogin04i' 'jrlogin05i' 'jrlogin06i' 'jrlogin07i' 'jrlogin08i' 'jrlogin09i' 'jrlogin10i' 'jrlogin11i' 'jrlogin12i'"
      },
      "JUSUF": {
        "hostname_base": "jsfl",
        "hostname_cpulimit": "login|vis",
        "hostname_all": "'jsfl01i' 'jsfl02i' 'jsfl03i' 'jsfl04i'"
      },
      "HDFML": {
        "hostname_base": "hdfmll",
        "hostname_cpulimit": "login|vis",
        "hostname_all": "'hdfmll01i' 'hdfmll02i'"
      },
      "DEEP": {
        "hostname_base": "deepv",
        "hostname_cpulimit": "login|vis",
        "hostname_all": "'deepv'"
      }
    }
  }

  stage_hub_replacements = {
    "production": {
      "coec": {
        "remote_port": "25480",
      },
      "coeraise": {
        "remote_port": "25482",
      },
      "euroccgcs": {
        "remote_port": "25486",
      },
      "juniq": {
        "remote_port": "25488",
      },
      "jupyterjsc": {
        "remote_port": "25490",
      },
      "portalgauss": {
        "remote_port": "25492",
      }
    },
    "staging": {
      "coec": {
        "remote_port": "25481",
      },
      "coeraise": {
        "remote_port": "25483",
      },
      "dev1": {
        "remote_port": "25484",
      },
      "dev2": {
        "remote_port": "25485",
      },
      "euroccgcs": {
        "remote_port": "25487",
      },
      "juniq": {
        "remote_port": "25489",
      },
      "jupyterjsc": {
        "remote_port": "25491",
      },
      "portalgauss": {
        "remote_port": "25493",
      }
    }
  }

  async def additional_replacements(spawner):
    stage = os.environ.get("STAGE", "undefined")
    hub = spawner.jupyterhub_name
    system = spawner.user_options.get("system", "undefined")
    ret = additional_replacements_base.copy()
    remote_port_replaces = stage_hub_replacements.get(stage, {}).get(hub, {})
    if remote_port_replaces:
      ret["system"][system].update(remote_port_replaces)
    else:
      spawner.log.error(f"Remote port for {hub}-{system} (hub-system) not defined.")
    return ret

  system_config = {
    "JURECA": {
      "site_url": "https://zam2125.zam.kfa-juelich.de:9112/JURECA/rest/core",
      "interactive_partitions": {
        "LoginNode": "jureca??.fz-juelich.de"
      }
    },
    "JUWELS": {
      "site_url": "https://zam2125.zam.kfa-juelich.de:9112/JUWELS/rest/core",
      "interactive_partitions": {
        "LoginNode": "juwels0?.fz-juelich.de",
        "LoginNodeVis": "juwelsvis??.fz-juelich.de",
        "LoginNodeBooster": "juwels2?.fz-juelich.de"
      }
    },
    "JUSUF": {
      "site_url": "https://zam2125.zam.kfa-juelich.de:9112/JUSUF/rest/core",
      "interactive_partitions": {
        "LoginNode": "jusuf?.fz-juelich.de"
      }
    },
    "HDFML": {
      "site_url": "https://zam2125.zam.kfa-juelich.de:9112/HDFML/rest/core",
      "interactive_partitions": {
        "LoginNode": "hdfmll??.fz-juelich.de"
      }
    },
    "DEEP": {
      "site_url": "https://zam2125.zam.kfa-juelich.de:9112/DEEP/rest/core",
      "interactive_partitions": {
        "LoginNode": "deep.fz-juelich.de"
      }
    }
  }

  async def unicore_site_url(spawner):
    system = spawner.user_options.get("system", "None")
    url = system_config.get(system, {}).get("site_url", False)
    if not url:
      raise Exception(f"URL for system {system} not configured. Available systems: {list(site_urls.keys())}")
    return url

  async def transport_kwargs(spawner):
    auth_state = await spawner.user.get_auth_state()
    credential = pyunicore.credentials.OIDCToken(auth_state["access_token"], None)
    transport_kwargs = {
        "credential": credential,
        # "verify": "/mnt/unicore/cert.crt",
        "verify": False,
        "timeout": 30
    }
    spawner.log.info(f"{spawner._log_name} - Unicore transport kwargs: {transport_kwargs}")
    return transport_kwargs

  async def transport_preferences(spawner):
    account = spawner.user_options.get("account", False)
    project = spawner.user_options.get("project", False)
    if account and project:
      preference = f"uid:{account},group:{project}"
      spawner.log.info(f"{spawner._log_name} - Set preference: {preference}")
      return preference
    else:
      spawner.log.warning(f"{spawner._log_name} - account ({account}) or project ({project}) not set in user_options ({spawner.user_options}). Do not set preferences in UNICORE transport.")
      return False


  c.UnicoreSpawner.unicore_internal_forwarding = False
  c.UnicoreSpawner.job_description = job_description
  c.UnicoreSpawner.additional_replacements = additional_replacements
  c.UnicoreSpawner.unicore_site_url = unicore_site_url
  c.UnicoreSpawner.unicore_transport_kwargs = transport_kwargs
  c.UnicoreSpawner.unicore_transport_preferences = transport_preferences
  c.UnicoreSpawner.unicore_job_delete = False

