outpostConfig: |-
  import logging
  import os
  import shutil
  import yaml

  from kubernetes import client
  from kubernetes import config
  from kubespawner import KubeSpawner

  ##########
  # Logging
  ##########
  class HealthCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          return record.getMessage().find("/ping") == -1

  class EventsCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          if record.funcName in ["_watch_and_update"]:
              return False
          return True

  # Remove health from application server logs
  logging.getLogger("uvicorn.access").addFilter(HealthCheckFilter())

  logger_name = os.environ.get("LOGGER_NAME", "JupyterHubOutpost")
  logging.getLogger(logger_name).addFilter(EventsCheckFilter())

  logged_logger_name = os.environ.get("LOGGER_NAME", "Outpost")
  c.JupyterHubOutpost.log_format = f"%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d {logged_logger_name} %(name)s %(module)s:%(lineno)d]%(end_color)s %(message)s"

  ##########
  # Flavors
  ##########

  # Used to send update of the current usage to JHub during start/stop
  async def flavors_update_token(jupyterhub_name):
      token = os.environ.get(f"FLAVOR_{jupyterhub_name.upper()}_AUTH_TOKEN", "")
      if not token:
          raise Exception(f"Flavor auth token for {jupyterhub_name} not configured.")
      return token

  # Specific flavors for users, or disable it completely for specific users
  async def flavors_per_user(outpost, jupyterhub_name, authentication):
      import copy
      hub_flavors = await outpost.get_flavors(jupyterhub_name)
      
      with open("/mnt/flavors/flavors.yaml", "r") as f:
          flavor_config = yaml.full_load(f)

      user_sets = []
      # check if the given user is part of any user set
      for key, value in flavor_config.get("users", {}).items():
          if jupyterhub_name in value.get("hubs", []):
              if authentication.get('name', 'unknown_user') in value.get("usernames", []):
                  user_sets.append((key, value.get("weight", 0)))
              else:
                  for user_group in authentication.get('groups', []):
                      if user_group in value.get("groups", []):
                          user_sets.append((key, value.get("weight", 0)))
                          break
      
      if len(user_sets) == 0:
          return hub_flavors

      user_sets = sorted(
          user_sets,
          key=lambda x: x[1]
      )
      # sorted sorts ascending, we're using weight, so we use the last element
      # with the biggest weight
      user_set = user_sets[-1][0]
      
      # When "disable" is true, we return an empty dict for this uset_set
      authentication_log = copy.deepcopy(authentication)
      if "access_token" in authentication_log.keys():
          del authentication_log["access_token"]
      if flavor_config.get("users", {}).get(user_set, {}).get("disable", False):
          outpost.log.debug(f"Specific user not allowed to use any flavors. Userset {user_set}. Used authentication dict: {authentication_log}")
          return {}

      outpost.log.debug(f"Use user specific flavor userset {user_set}. Used authentication dict: {authentication_log}")

      # Copy default flavors for this hub
      all_flavors = await outpost.get_flavors(None)
      user_flavors = {}
      
      for flavor in flavor_config.get("users", {}).get(user_set, {}).get("flavors", []):
          if flavor in all_flavors.keys():
              user_flavors[flavor] = all_flavors[flavor]
          else:
              outpost.log.warning(f"Unknown flavor for user set {user_set}: {flavor}")
      
      for flavorName, overrideDict in flavor_config.get("users", {}).get(user_set, {}).get("flavorsOverride", {}).items():
          if flavorName not in user_flavors.keys():
              outpost.log.warning(f"Do not override {flavorName} for user set {user_set}. Flavor not part of flavors list.")
              continue
          for overrideKey, overrideValue in overrideDict.items():
              user_flavors[flavorName][overrideKey] = overrideValue
      
      return user_flavors

  # Default flavors per JupyterHub
  async def flavors_per_hub(jupyterhub_name):
      with open("/mnt/flavors/flavors.yaml", "r") as f:
          flavor_config = yaml.full_load(f)
      
      # If no hub is defined, return all available flavors
      if not jupyterhub_name:
          return flavor_config.get("flavors", {})
      
      jupyterhub_sets = []
      # check if the given jupyterhub_name is part of any jhub set
      for key, value in flavor_config.get("hubs", {}).items():
          if jupyterhub_name in value.get("jupyterhubs", []):
              jupyterhub_sets.append((key, value.get("weight", 0)))
      
      # jupyterhub_name is not allowed to use any flavors 
      if len(jupyterhub_sets) == 0:
          return {}
      
      jupyterhub_sets = sorted(
          jupyterhub_sets,
          key=lambda x: x[1]
      )
      # sorted sorts ascending, we're using weight, so we use the last element
      # with the biggest weight
      jupyterhub_set = jupyterhub_sets[-1][0]
      
      hub_specific_flavors = {}
      for flavorName in flavor_config.get("hubs", {}).get(jupyterhub_set, {}).get("flavors", []):
          if flavorName in flavor_config.get("flavors", {}).keys():
              hub_specific_flavors[flavorName] = flavor_config["flavors"][flavorName]
      
      return hub_specific_flavors

  def get_flavor_resources(flavor):
      with open("/mnt/flavors/flavors.yaml", "r") as f:
          flavor_config = yaml.full_load(f)
      
      if flavor not in flavor_config.get("flavors", {}).keys():
          raise Exception(f"Flavor {flavor} not configured. Abort start.")
      
      return flavor_config["flavors"][flavor].get("resources", {})

  c.JupyterHubOutpost.flavors_update_token = flavors_update_token
  c.JupyterHubOutpost.flavors = flavors_per_hub
  c.JupyterHubOutpost.user_flavors = flavors_per_user
  c.JupyterHubOutpost.flavors_undefined_max = 0


  ##########
  # KubeSpawner
  ##########
  c.JupyterHubOutpost.spawner_class = KubeSpawner

  c.KubeSpawner.image = "registry.jsc.fz-juelich.de/jupyterjsc/k8s/images/user-jupyterlab:lmod-4.0.9"
  c.KubeSpawner.image_pull_policy = "Always"
  c.KubeSpawner.tolerations = [
      {"key": "usernode", "value": "true", "effect": "NoExecute"}
  ]
  # We set all the labels we want in extra_labels, so overwrite common_labels
  c.KubeSpawner.common_labels = {}

  c.KubeSpawner.secret_name_template = "tls-{userid}-{servername}"
  c.KubeSpawner.start_timeout = 600
  c.KubeSpawner.fs_gid = 100
  c.KubeSpawner.container_security_context = {
      "allowPrivilegeEscalation": True,
      "privileged": True,
      "capabilities": {
          "add": ["CAP_SYS_ADMIN"]
      }
  }

  c.KubeSpawner.extra_pod_config = {
      "restartPolicy": "Always",
  }
  c.KubeSpawner.delete_grace_period = 20
  c.KubeSpawner.lifecycle_hooks = {
      "preStop": {
          "exec": {
              "command": ["/bin/sh", "-c", "kill $(ps x | grep '[j]upyterhub-singleuser' | awk '{print $1}')"]
          }
      }
  }


  ##########
  # KubeSpawner prespawn hook
  ##########

  def pre_spawn_hook(spawner):
      hub = spawner.extra_labels.get("app", "jupyter")
      stage = os.environ.get("STAGE", "staging")
      userid = str(spawner.user.id)
      soft = "25g"
      hard = "30g"

      token = os.environ.get("STORAGE_MANAGER_TOKEN", None)
      storage_url = os.environ.get("STORAGE_MANAGER_URL", None)
      if storage_url and token:
          request_header = {
              "Content-Type": "application/json",
              "Accept": "application/json",
          }
          body = {
            "stage": stage,
            "service": hub,
            "user_id": userid,
            "softlimit": soft,
            "hardlimit": hard
          }
          try:
              r = requests.post(f"{storage_url}?token={token}", headers=request_header, json=body)
              r.raise_for_status()
              storage_info = r.json()
              used = storage_info.get("human", {}).get("used", "0G")
              summary = ""
              details = ""
              red = False
              orange = False
              if int(storage_info.get("bytes", {}).get("used", "0")) >= int(storage_info.get("bytes", {}).get("hard")):
                  red = True
                  summary = f"Hard limit of disk quota exceeded ({used} / {soft}). Your container might not start for this reason. Please contact support (see button in footer) to solve this."
                  details = f"You're allowed to use up to {soft} disk space. Please keep your disk usage below this threshold."
              elif int(storage_info.get("bytes", {}).get("used", "0")) > int(storage_info.get("bytes", {}).get("soft")):
                  summary = f"Soft limit of disk quota exceeded ({used} / {soft}). Please remove some files to stay below {soft}. Your container might not start for this reason. Please contact support (see button in footer) to solve this."
                  details = f"It's possible to use up to {hard} for maximum 7 days. Afterwards, file creation in your directory is impossible, which may prevent a service start."
                  orange = True
              else:
                  summary = f"Disk quota status: {used} / {soft}"
                  details = f"It's possible to use up to {hard} for maximum 7 days. Afterwards, file creation in your directory is impossible, which may prevent a service start."

              if red:
                  msg = f"<details><summary><span style=\"color:red;\">{summary}</span></summary>{details}</details>"
              elif orange:
                  msg = f"<details><summary><span style=\"color:darkorange;\">{summary}</span></summary>{details}</details>"
              else:
                  msg = f"<details><summary>{summary}</summary>{details}</details>"
              event = {
                  "progress": "10",
                  "html_message": msg,
                  "failed": False
              }
              def sync_send_event(spawner, event):
                  loop = asyncio.new_event_loop()

                  async def wait_for_future(future):
                      return await future

                  def t_send_event(loop, spawner, event):
                      asyncio.set_event_loop(loop)
                      loop.run_until_complete(wait_for_future(spawner._outpostspawner_send_event(event)))
                      
                  t = Thread(target=t_send_event, args=(loop, spawner, event))
                  t.start()
                  t.join()

              sync_send_event(spawner, event)
          except:
              spawner.log.exception(f"{spawner._log_name} - Could not get storage information")
      else:
          spawner.log.warning(f"{spawner._log_name} - Storage Manager env variables not set")

      if "usermount_path" in spawner.user_options:
          del spawner.user_options["usermount_path"]

      # Create registry secret, if `dockerregistry` is part of user_options
      dockerregistry = spawner.user_options.get("dockerregistry", "")
      if dockerregistry:
          try:
              config.load_incluster_config()
              v1 = client.CoreV1Api()
              image_pull_secret_name = f"{spawner.pod_name}-reg"
              secret_manifest = {
                  "apiVersion": "v1",
                  "kind": "Secret",
                  "type": "kubernetes.io/dockerconfigjson",
                  "metadata": {
                      "name": image_pull_secret_name,
                      "resourceversion": "v1",
                      "labels": {
                          "pod": spawner.pod_name
                      }
                  },
                  "data": {
                      ".dockerconfigjson": dockerregistry
                  },
              }
              try:
                  v1.read_namespaced_secret(name=image_pull_secret_name, namespace=spawner.namespace)
              except:
                  pass
              else:
                  spawner.log.info(f"{spawner._log_name} - Delete pre existing registry secret {image_pull_secret_name}")
                  v1.delete_namespaced_secret(name=image_pull_secret_name, namespace=spawner.namespace)
              v1.create_namespaced_secret(body=secret_manifest, namespace=spawner.namespace)
              image_pull_secrets = getattr(spawner, "image_pull_secrets", [])
              if not image_pull_secrets:
                  image_pull_secrets = [image_pull_secret_name]
              elif type(image_pull_secrets) == list:
                  image_pull_secrets.append(image_pull_secret_name)
              elif type(image_pull_secrets) == str:
                  image_pull_secrets = [image_pull_secrets] + [image_pull_secret_name]
              else:
                  image_pull_secrets = [image_pull_secret_name]
              spawner.image_pull_secrets = image_pull_secrets
          except:
              spawner.log.exception(f"{spawner._log_name} - Could not create dockerregistry secret")

  c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  ##########
  # KubeSpawner post_stop_hook
  ##########

  def post_stop_hook(spawner):
      config.load_incluster_config()
      v1 = client.CoreV1Api()
      image_pull_secret_name = f"{spawner.pod_name}-reg"
      try:
          v1.read_namespaced_secret(name=image_pull_secret_name, namespace=spawner.namespace)
      except:
          pass
      else:
          spawner.log.info(f"{spawner._log_name} - Delete registry secret {image_pull_secret_name}")
          v1.delete_namespaced_secret(name=image_pull_secret_name, namespace=spawner.namespace)

  c.KubeSpawner.post_stop_hook = post_stop_hook

  ##########
  # KubeSpawner profiles
  ##########

  def get_profile(spawner):
      profile = spawner.user_options.get("profile", "JupyterLab/3.6")
      if "/" in profile:
          profile = '-'.join(profile.split("/")[1:])
      profile = profile.replace(".", "-")
      return profile


  def get_profile_volume_mounts(spawner):
      profile = get_profile(spawner)
      if profile == "custom":
          if "userdata_path" in spawner.user_options:
              userdata_mount = [
                  {
                      "name": "nfs-userdata",
                      "mountPath": spawner.user_options["userdata_path"],
                      "readOnly": False 
                  }
              ]
              return userdata_mount
          else:
              return []
      else:
          common_mounts = [
              {
                  "name": "nfs-software", 
                  "mountPath": "/p/software/jsccloud", 
                  "readOnly": True
              },
              {
                  "name": "uftp", 
                  "mountPath": "/tmp/custom/uftp.py",
                  "subPath": "uftp.py"
              },
              {
                  "name": "nfs-userhome",
                  "mountPath": "/home/jovyan",
                  "readOnly": False 
              }
          ]
          profile_mounts = [
          {
                  "name": f"modules-{profile}", 
                  "mountPath": "/tmp/custom/load_jupyter_version.sh", 
                  "subPath": "load_jupyter_version.sh"
              },
              {
                  "name": f"modules-user-{profile}", 
                  "mountPath": "/tmp/custom/load_jupyter_modules.sh",
                  "subPath": "load_jupyter_modules.sh"
              }
          ]
          return common_mounts + profile_mounts

  def get_profile_volumes(spawner, jupyterhub_name):
      profile = get_profile(spawner)
      stage = os.environ.get("STAGE", "staging")

      if profile == "custom":
          if "userdata_path" in spawner.user_options:
              userdata_volume = [
              {
                  "name": "nfs-userdata", 
                  "hostPath": {
                      # userid gets expanded by KubeSpawner, do not use mapped 
                      # hub name here, each hub uses it's own home dir.
                      "path": f"/p/home/jusers/{stage}/{jupyterhub_name}/{{userid}}", 
                      "type": ""
                  }
              }
              ]
              return userdata_volume
          else:
              return []
      else:   
          map_hub_names = {
              "jupyterjsc": "defaulthub",
              "coec": "defaulthub",
              "coeraise": "defaulthub",
              "portalgauss": "defaulthub",
              "euroccgcs": "defaulthub",
              "dev1": "defaulthub",
              "dev2": "defaulthub"
          }
          hub = map_hub_names.get(jupyterhub_name, jupyterhub_name)

          common_volumes = [
              {
                  "name": "nfs-software", 
                  "hostPath": {
                      "path": "/p/software/jsccloud", 
                      "type": ""
                  }
              },
              {
                  "name": "uftp", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": "uftp"
                  }
              },
              {
                  "name": "nfs-userhome", 
                  "hostPath": {
                      # userid gets expanded by KubeSpawner, do not use mapped 
                      # hub name here, each hub uses it's own home dir.
                      "path": f"/p/home/jusers/{stage}/{jupyterhub_name}/{{userid}}", 
                      "type": ""
                  }
              }
          ]
          profile_volumes = [
              {
                  "name": f"modules-{profile}", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": f"modules-{hub}-{profile}"
                  }
              },
              {
                  "name": f"modules-user-{profile}", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": f"modules-user-{hub}-{profile}"
                  }
              }
          ]
          return common_volumes + profile_volumes

  def get_node_selector(spawner, jupyterhub_name, resources, flavor):
      from kubernetes import client
      from kubernetes import config
      import json

      config.load_incluster_config()
      core = client.CoreV1Api()
      flavor_mem_request = resources.get("mem_guarantee", "0M")
      spawner.log.info(f"{spawner._log_name} - {flavor_mem_request}M memory for {flavor} ({jupyterhub_name}) requested")
      base_node_name = "prod-usernode-"
      
      first_node = True
      outpost_mem_blocked = 1024
      for i in range(0, 51):
          node_name = f"{base_node_name}{i}"
          try:
              node = core.read_node_status(node_name)
          except:
              spawner.log.info(f"{spawner._log_name} - Could not load node {node_name}")
              continue
          requested_mem_mi_s = json.loads(node.metadata.annotations.get("management.cattle.io/pod-requests", '{}')).get("memory", "0Mi")
          total_mem_ki_s = node.status.allocatable.get("memory", "0Ki") # 8128016
          free_mem_mi = int(total_mem_ki_s[:-2])/1024 - int(requested_mem_mi_s[:-2])
          free_mem_m = free_mem_mi*1.04858
          if first_node:
              first_node = False
              spawner.log.info(f"{spawner._log_name} - {node_name} - keep {outpost_mem_blocked} free for JupyterHub Outpost")
              free_mem_m = free_mem_m - outpost_mem_blocked
          spawner.log.info(f"{spawner._log_name} - {node_name} has {free_mem_m}M free memory.")
          if free_mem_m > int(flavor_mem_request[:-1]):
              spawner.log.info(f"{spawner._log_name} - Use {node_name}")
              return {"kubernetes.io/hostname": node_name}
          else:
              spawner.log.info(f"{spawner._log_name} - Do not use {node_name}")
      spawner.log.warning("{spawner._log_name} - Use kubernetes scheduler to place node")
      return {"usernode": "true"}

  async def profile_list(spawner):
      # We're loading this config file for each Spawner object
      # Therefore, it's fine to just create one profile
      jupyterhub_name = spawner.jupyterhub_name
      profile = get_profile(spawner)
      flavor = spawner.user_options.get("flavor", "default")
      resources = get_flavor_resources(flavor)

      display_name = {
          "custom": "JupyterLab - custom",
          "3-6": "JupyterLab - 3.6",
          "4-2": "JupyterLab - 4.2"
      }
      slug = {
          "custom": "JupyterLab/custom",
          "3-6": "JupyterLab/3.6",
          "4-2": "JupyterLab/4.2"
      }

      if profile == "custom":
          return [
              {
                  "display_name": "JupyterLab - custom",
                  "slug": "JupyterLab/custom",
                  "kubespawner_override": {
                      "image": spawner.user_options["image"],
                      "volume_mounts": get_profile_volume_mounts(spawner),
                      "volumes": get_profile_volumes(spawner, jupyterhub_name),
                      "node_selector": get_node_selector(spawner, jupyterhub_name, resources, flavor),
                      "cpu_guarantee": resources["cpu_guarantee"],
                      "cpu_limit": resources["cpu_limit"],
                      "mem_guarantee": resources["mem_guarantee"],
                      "mem_limit": resources["mem_limit"]
                  }
              }
          ]
      else:
          return [
              {
                  "display_name": display_name.get(profile, "JupyterLab - 3.6"),
                  "slug": slug.get(profile, "JupyterLab/3.6"),
                  "kubespawner_override": {
                      "volume_mounts": get_profile_volume_mounts(spawner),
                      "volumes": get_profile_volumes(spawner, jupyterhub_name),
                      "node_selector": get_node_selector(spawner, jupyterhub_name, resources, flavor),
                      "cpu_guarantee": resources["cpu_guarantee"],
                      "cpu_limit": resources["cpu_limit"],
                      "mem_guarantee": resources["mem_guarantee"],
                      "mem_limit": resources["mem_limit"]
                  }
              }
          ]

  c.KubeSpawner.profile_list = profile_list
