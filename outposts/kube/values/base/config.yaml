outpostConfig: |-
  import logging
  import os
  import shutil

  from kubespawner import KubeSpawner

  ##########
  # Logging
  ##########
  class HealthCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          return record.getMessage().find("/ping") == -1

  class EventsCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          if record.funcName in ["_watch_and_update"]:
              return False
          return True

  # Remove health from application server logs
  logging.getLogger("uvicorn.access").addFilter(HealthCheckFilter())

  logger_name = os.environ.get("LOGGER_NAME", "JupyterHubOutpost")
  logging.getLogger(logger_name).addFilter(EventsCheckFilter())

  logged_logger_name = os.environ.get("LOGGER_NAME", "Outpost")
  c.JupyterHubOutpost.log_format = f"%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d {logged_logger_name} %(name)s %(module)s:%(lineno)d]%(end_color)s %(message)s"

  async def user_specific_flavors(outpost, jupyterhub_name, authentication):
      import copy
      default_flavors = await outpost.get_flavors(jupyterhub_name)
      specific_flavors = copy.deepcopy(default_flavors)
      # HIFIS users can only start, if there are less than 2 Labs running
      outpost.log.info(f"Return specific flavors for user {authentication.get('name', 'unknown_user')} ({authentication.get('groups', [])})")
      if "HIFIS" in authentication.get("groups", []):
          for key in specific_flavors:
              specific_flavors[key]["max"] = 2
      return specific_flavors

  c.JupyterHubOutpost.user_flavors = user_specific_flavors

  ##########
  # Flavors
  ##########
  m1_default = {
      "max": -1,
      "weight": 10,
      "display_name": "2GB RAM, 1VCPU, 5 days",
      "description": "JupyterLab will run for max 5 days with 2GB RAM and 1VCPU.",
      "runtime": {
          "days": 5
      }
  }
  l1_default = {
      "max": 4,
      "weight": 9,
      "display_name": "4GB RAM, 1VCPUs, 2 days",
      "description": "JupyterLab will run for max 2 days with 4GB RAM and 1VCPUs.",
      "runtime": {
          "days": 2
      }
  }
  l2_default = {
      "max": 2,
      "weight": 3,
      "display_name": "8GB RAM, 2VCPUs, 10 hours",
      "description": "JupyterLab will run for max 10 hours with 8GB RAM and 2VCPUs.",
      "runtime": {
          "hours": 10
      }
  }
  all_flavors = {
      "jupyterjsc": {
          "m1": m1_default,
          "l1": l1_default,
          "l2": l2_default
      },
      "juniq": {
          "m1": m1_default,
          "l1": l1_default,
          "l2": l2_default
      },
      "coeraise": {
          "m1": m1_default,
          "l1": l1_default
      },
      "coec": {
          "m1": m1_default,
          "l1": l1_default
      },
      "portalgauss": {
          "m1": m1_default,
          "l1": l1_default
      },
      "euroccgcs": {
          "m1": m1_default,
          "l1": l1_default
      },
      "dev1": {
          "m1": m1_default,
          "l1": l1_default
      },
      "dev2": {
          "m1": m1_default,
          "l1": l1_default
      }
  }

  # Guarantee is a bit less, to allow system pods run on the usernodes
  # as well
  all_flavor_resources = {
      "m1": {
          "cpu_guarantee": 0.2,
          "cpu_limit": 1,
          "mem_guarantee": "1536M",
          "mem_limit": "2048M"
      },
      "l1": {
          "cpu_guarantee": 0.2,
          "cpu_limit": 1,
          "mem_guarantee": "3584M",
          "mem_limit": "4096M"
      },
      "l2": {
          "cpu_guarantee": 0.2,
          "cpu_limit": 2,
          "mem_guarantee": "7680M",
          "mem_limit": "8192M"
      }
  }

  async def flavors_update_token(jupyterhub_name):
      token = os.environ.get(f"FLAVOR_{jupyterhub_name.upper()}_AUTH_TOKEN", "")
      if not token:
          raise Exception(f"Flavor auth token for {jupyterhub_name} not configured.")
      return token

  async def flavors(jupyterhub_name):
      return all_flavors.get(jupyterhub_name, {})

  def get_flavor_resources(flavor):
      if flavor not in all_flavor_resources.keys():
          raise Exception(f"Flavor {flavor} not configured. Abort start.")
      return all_flavor_resources[flavor]

  c.JupyterHubOutpost.flavors_update_token = flavors_update_token
  c.JupyterHubOutpost.flavors = flavors
  c.JupyterHubOutpost.flavors_undefined_max = 0


  ##########
  # KubeSpawner
  ##########
  c.JupyterHubOutpost.spawner_class = KubeSpawner

  c.KubeSpawner.image = "registry.jsc.fz-juelich.de/jupyterjsc/k8s/images/user-jupyterlab:lmod-4.0.5"
  c.KubeSpawner.image_pull_policy = "Always"
  c.KubeSpawner.tolerations = [
      {"key": "usernode", "value": "true", "effect": "NoExecute"}
  ]
  # We set all the labels we want in extra_labels, so overwrite common_labels
  c.KubeSpawner.common_labels = {}

  c.KubeSpawner.secret_name_template = "tls-{userid}-{servername}"
  c.KubeSpawner.start_timeout = 600
  c.KubeSpawner.fs_gid = 100
  c.KubeSpawner.container_security_context = {
      "allowPrivilegeEscalation": True,
      "privileged": True,
      "capabilities": {
          "add": ["CAP_SYS_ADMIN"]
      }
  }

  c.KubeSpawner.extra_pod_config = {
      "restartPolicy": "Always",
  }
  c.KubeSpawner.delete_grace_period = 20
  c.KubeSpawner.lifecycle_hooks = {
      "preStop": {
          "exec": {
              "command": ["/bin/sh", "-c", "kill $(ps x | grep '[j]upyterhub-singleuser' | awk '{print $1}')"]
          }
      }
  }


  ##########
  # KubeSpawner prespawn hook
  ##########

  def pre_spawn_hook(spawner):
      userhome_base = "/p/home/jusers"
      hub = spawner.extra_labels.get("app", "jupyter")
      stage = os.environ.get("STAGE", "staging")
      userid = str(spawner.user.id)
      profile = spawner.user_options.get("profile", "")
      try:
          service = profile.split("/")[0].lower()
      except:
          spawner.log.exception(f"{spawner._log_name} - Use default service: jupyterlab for skeleton dir")
          service = "jupyterlab"

      userhome_dir = os.path.join(userhome_base, hub, stage, userid)
      userhome_skel = f"{userhome_base}/skel/{service}"
      userhome_uid = 1000
      userhome_gid = 1000

      def copy_file(src, dst):
          if os.path.isfile(src):
              shutil.copyfile(src=src, dst=dst)
              shutil.chown(dst, userhome_uid, userhome_gid)

      def copy_dir(src, dst):
          shutil.copytree(src=src, dst=dst)
          for dirpath, dirnames, filenames in os.walk(dst):
              shutil.chown(dirpath, userhome_uid, userhome_gid)
              for filename in filenames:
                  shutil.chown(os.path.join(dirpath, filename), userhome_uid, userhome_gid)

      if not os.path.exists(userhome_dir):
          spawner.log.info(f"{spawner._log_name} - Create home directory {userhome_dir}")
          os.makedirs(userhome_skel, exist_ok=True)
          copy_dir(userhome_skel, userhome_dir)
      else:
          # Update motd
          copy_file(f"{userhome_skel.rstrip('/')}/.motd", f"{userhome_dir.rstrip('/')}/.motd")
          shutil.rmtree(f"{userhome_dir.rstrip('/')}/.motd.d", ignore_errors=True)
          copy_dir(f"{userhome_skel.rstrip('/')}/.motd.d", f"{userhome_dir.rstrip('/')}/.motd.d")

      if "usermount_path" in spawner.user_options:
          del spawner.user_options["usermount_path"]

  c.KubeSpawner.pre_spawn_hook = pre_spawn_hook


  ##########
  # KubeSpawner profiles
  ##########

  def get_profile(spawner):
      profile = spawner.user_options.get("profile", "JupyterLab/3.6")
      if "/" in profile:
          profile = '-'.join(profile.split("/")[1:])
      profile = profile.replace(".", "-")
      return profile


  def get_profile_volume_mounts(spawner):
      profile = get_profile(spawner)
      if profile == "custom":
          if "userdata_path" in spawner.user_options:
              userdata_mount = [
                  {
                      "name": "nfs-userdata",
                      "mountPath": spawner.user_options["userdata_path"],
                      "readOnly": False 
                  }
              ]
              return userdata_mount
          else:
              return []
      else:
          common_mounts = [
              {
                  "name": "nfs-software", 
                  "mountPath": "/p/software/jsccloud", 
                  "readOnly": True
              },
              {
                  "name": "uftp", 
                  "mountPath": "/tmp/custom/uftp.py",
                  "subPath": "uftp.py"
              },
              {
                  "name": "nfs-userhome",
                  "mountPath": "/home/jovyan",
                  "readOnly": False 
              }
          ]
          profile_mounts = [
          {
                  "name": f"modules-{profile}", 
                  "mountPath": "/tmp/custom/load_jupyter_version.sh", 
                  "subPath": "load_jupyter_version.sh"
              },
              {
                  "name": f"modules-user-{profile}", 
                  "mountPath": "/tmp/custom/load_jupyter_modules.sh",
                  "subPath": "load_jupyter_modules.sh"
              }
          ]
          return common_mounts + profile_mounts

  def get_profile_volumes(spawner, jupyterhub_name):
      profile = get_profile(spawner)
      stage = os.environ.get("STAGE", "staging")

      if profile == "custom":
          if "userdata_path" in spawner.user_options:
              userdata_volume = [
              {
                  "name": "nfs-userdata", 
                  "hostPath": {
                      # userid gets expanded by KubeSpawner, do not use mapped 
                      # hub name here, each hub uses it's own home dir.
                      "path": f"/p/home/jusers/{jupyterhub_name}/{stage}/{{userid}}", 
                      "type": ""
                  }
              }
              ]
              return userdata_volume
          else:
              return []
      else:   
          map_hub_names = {
              "jupyterjsc": "defaulthub",
              "coec": "defaulthub",
              "coeraise": "defaulthub",
              "portalgauss": "defaulthub",
              "euroccgcs": "defaulthub",
              "dev1": "defaulthub",
              "dev2": "defaulthub"
          }
          hub = map_hub_names.get(jupyterhub_name, jupyterhub_name)

          common_volumes = [
              {
                  "name": "nfs-software", 
                  "hostPath": {
                      "path": "/p/software/jsccloud", 
                      "type": ""
                  }
              },
              {
                  "name": "uftp", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": "uftp"
                  }
              },
              {
                  "name": "nfs-userhome", 
                  "hostPath": {
                      # userid gets expanded by KubeSpawner, do not use mapped 
                      # hub name here, each hub uses it's own home dir.
                      "path": f"/p/home/jusers/{jupyterhub_name}/{stage}/{{userid}}", 
                      "type": ""
                  }
              }
          ]
          profile_volumes = [
              {
                  "name": f"modules-{profile}", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": f"modules-{hub}-{profile}"
                  }
              },
              {
                  "name": f"modules-user-{profile}", 
                  "configMap": {
                      "defaultMode": 292,
                      "name": f"modules-user-{hub}-{profile}"
                  }
              }
          ]
          return common_volumes + profile_volumes

  def get_node_selector(spawner, jupyterhub_name, resources, flavor):
      from kubernetes import client
      from kubernetes import config
      import json

      config.load_incluster_config()
      core = client.CoreV1Api()
      flavor_mem_request = resources.get("mem_guarantee", "0M")
      spawner.log.info(f"{spawner._log_name} - {flavor_mem_request}M memory for {flavor} ({jupyterhub_name}) requested")
      base_node_name = "usernode-"
      
      first_node = True
      outpost_mem_blocked = 1024
      for i in range(0, 51):
          node_name = f"{base_node_name}{i}"
          try:
              node = core.read_node_status(node_name)
          except:
              spawner.log.info(f"{spawner._log_name} - Could not load node {node_name}")
              continue
          requested_mem_mi_s = json.loads(node.metadata.annotations.get("management.cattle.io/pod-requests", '{}')).get("memory", "0Mi")
          total_mem_ki_s = node.status.allocatable.get("memory", "0Ki") # 8128016
          free_mem_mi = int(total_mem_ki_s[:-2])/1024 - int(requested_mem_mi_s[:-2])
          free_mem_m = free_mem_mi*1.04858
          if first_node:
              first_node = False
              spawner.log.info(f"{spawner._log_name} - {node_name} - keep {outpost_mem_blocked} free for JupyterHub Outpost")
              free_mem_m = free_mem_m - outpost_mem_blocked
          spawner.log.info(f"{spawner._log_name} - {node_name} has {free_mem_m}M free memory.")
          if free_mem_m > int(flavor_mem_request[:-1]):
              spawner.log.info(f"{spawner._log_name} - Use {node_name}")
              return {"kubernetes.io/hostname": node_name}
          else:
              spawner.log.info(f"{spawner._log_name} - Do not use {node_name}")
      spawner.log.warning("{spawner._log_name} - Use kubernetes scheduler to place node")
      return {"usernode": "true"}

  async def profile_list(spawner):
      # We're loading this config file for each Spawner object
      # Therefore, it's fine to just create one profile
      jupyterhub_name = spawner.jupyterhub_name
      profile = get_profile(spawner)
      flavor = spawner.user_options.get("flavor", "default")
      resources = get_flavor_resources(flavor)

      if profile == "custom":
          return [
              {
                  "display_name": "JupyterLab - custom",
                  "slug": "JupyterLab/custom",
                  "kubespawner_override": {
                      "image": spawner.user_options["image"],
                      "volume_mounts": get_profile_volume_mounts(spawner),
                      "volumes": get_profile_volumes(spawner, jupyterhub_name),
                      "node_selector": get_node_selector(spawner, jupyterhub_name, resources, flavor),
                      "cpu_guarantee": resources["cpu_guarantee"],
                      "cpu_limit": resources["cpu_limit"],
                      "mem_guarantee": resources["mem_guarantee"],
                      "mem_limit": resources["mem_limit"]
                  }
              }
          ]
      else:
          return [
              {
                  "display_name": "JupyterLab - 3.6",
                  "slug": "JupyterLab/3.6",
                  "kubespawner_override": {
                      "volume_mounts": get_profile_volume_mounts(spawner),
                      "volumes": get_profile_volumes(spawner, jupyterhub_name),
                      "node_selector": get_node_selector(spawner, jupyterhub_name, resources, flavor),
                      "cpu_guarantee": resources["cpu_guarantee"],
                      "cpu_limit": resources["cpu_limit"],
                      "mem_guarantee": resources["mem_guarantee"],
                      "mem_limit": resources["mem_limit"]
                  }
              }
          ]

  c.KubeSpawner.profile_list = profile_list

