outpostConfig: |-
  import logging
  import os
  import shutil

  from kubespawner import KubeSpawner

  class HealthCheckFilter(logging.Filter):
      def filter(self, record: logging.LogRecord) -> bool:
          return record.getMessage().find("/ping") == -1

  # Remove health from application server logs
  logging.getLogger("uvicorn.access").addFilter(HealthCheckFilter()) 

  logged_logger_name = os.environ.get("LOGGER_NAME", "Outpost")
  c.JupyterHubOutpost.log_format = f"%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d {logged_logger_name} %(name)s %(module)s:%(lineno)d]%(end_color)s %(message)s"


  async def flavors_update_token(jupyterhub_name):
      token = os.environ.get(f"FLAVOR_{jupyterhub_name.upper()}_AUTH_TOKEN", "")
      if not token:
          raise Exception(f"Flavor auth token for {jupyterhub_name} not configured.")
      return token

  all_flavors = {
      "jupyterjsc": {
          "m1": {
              "max": -1,
              "weight": 10,
              "display_name": "2GB RAM, 1VCPU, 120 hours",
              "description": "JupyterLab will run for max 120 hours with 2GB RAM and 1VCPU."
          },
          "l1": {
              "max": -1,
              "weight": 9,
              "display_name": "4GB RAM, 1VCPUs, 12 hours",
              "description": "JupyterLab will run for max 12 hours with 4GB RAM and 1VCPUs."
          },
          "l2": {
              "max": 4,
              "weight": 2,
              "display_name": "8GB RAM, 2VCPUs, 6 hours",
              "description": "JupyterLab will run for max 6 hours with 8GB RAM and 2VCPUs."
          },
          "xl2": {
              "max": 2,
              "weight": 1,
              "display_name": "16GB RAM, 2VCPUs, 2 hours",
              "description": "JupyterLab will run for max 2 hours with 16GB RAM and 2VCPUs."
          },
      },
      "juniq": {
          "m1": {
              "max": -1,
              "weight": 10,
              "display_name": "2GB RAM, 1VCPU, 120 hours",
              "description": "JupyterLab will run for max 120 hours with 2GB RAM and 1VCPU."
          },
          "l1": {
              "max": -1,
              "weight": 9,
              "display_name": "4GB RAM, 1VCPUs, 12 hours",
              "description": "JupyterLab will run for max 12 hours with 4GB RAM and 1VCPUs."
          },
          "l2": {
              "max": 2,
              "weight": 2,
              "display_name": "8GB RAM, 2VCPUs, 6 hours",
              "description": "JupyterLab will run for max 6 hours with 8GB RAM and 2VCPUs."
          }
      },
      "coeraise": {
          "m1": {
              "max": -1,
              "weight": 10,
              "display_name": "2GB RAM, 1VCPU, 120 hours",
              "description": "JupyterLab will run for max 120 hours with 2GB RAM and 1VCPU."
          },
          "l1": {
              "max": -1,
              "weight": 9,
              "display_name": "4GB RAM, 1VCPUs, 12 hours",
              "description": "JupyterLab will run for max 12 hours with 4GB RAM and 1VCPUs."
          }
      },
      "coec": {
          "m1": {
              "max": -1,
              "weight": 10,
              "display_name": "2GB RAM, 1VCPU, 120 hours",
              "description": "JupyterLab will run for max 120 hours with 2GB RAM and 1VCPU."
          },
          "l1": {
              "max": -1,
              "weight": 9,
              "display_name": "4GB RAM, 1VCPUs, 12 hours",
              "description": "JupyterLab will run for max 12 hours with 4GB RAM and 1VCPUs."
          }
      },
      "portalgauss": {
          "m1": {
              "max": -1,
              "weight": 10,
              "display_name": "2GB RAM, 1VCPU, 120 hours",
              "description": "JupyterLab will run for max 120 hours with 2GB RAM and 1VCPU."
          },
          "l1": {
              "max": -1,
              "weight": 9,
              "display_name": "4GB RAM, 1VCPUs, 12 hours",
              "description": "JupyterLab will run for max 12 hours with 4GB RAM and 1VCPUs."
          }
      }
  }

  all_hub_groups = {
    "jupyterjsc": ["default", "HIFIS", "admin"],
    "juniq": ["default", "HIFIS", "admin"],
    "coec": ["default", "HIFIS", "admin"],
    "coeraise": ["default", "HIFIS", "admin"],
    "portalgauss": ["default", "HIFIS", "admin"]
  }

  async def flavors(jupyterhub_name):
      ret = {}
      for group in all_hub_groups.get(jupyterhub_name, []):
          ret[group] = all_flavors.get(jupyterhub_name, {})
      return ret

  c.JupyterHubOutpost.flavors_update_token = flavors_update_token
  c.JupyterHubOutpost.flavors = flavors


  c.JupyterHubOutpost.spawner_class = KubeSpawner

  c.KubeSpawner.image = "registry.jsc.fz-juelich.de/jupyterjsc/k8s/images/user-jupyterlab:lmod-4.0.3"
  c.KubeSpawner.image_pull_policy = "Always"
  c.KubeSpawner.scheduler_name = "usernode-scheduler"
  # Use scheduler instead of node_selector, which fills one node before 
  # spawning pods on the next one
  # c.KubeSpawner.node_selector = {"usernode": "true"}
  c.KubeSpawner.tolerations = [
      {"key": "usernode", "value": "true", "effect": "NoExecute"}
  ]

  c.KubeSpawner.cpu_guarantee = 0.1
  c.KubeSpawner.cpu_limit = 2
  c.KubeSpawner.mem_guarantee = "512M"
  c.KubeSpawner.mem_limit = "4096M"

  c.KubeSpawner.fs_gid = 100
  c.KubeSpawner.container_security_context = {
      "allowPrivilegeEscalation": True,
      "privileged": True,
      "capabilities": {
          "add": ["CAP_SYS_ADMIN"]
      }
  }

  # Might need longer delete grace periods for unmounting in production
  # c.KubeSpawner.delete_grace_period = 5
  c.KubeSpawner.lifecycle_hooks = {
      "preStop": {
          "exec": {
              "command": ["/bin/sh", "-c", "kill $(ps x | grep '[t]imeout .* jupyterhub-singleuser' | awk '{print $1}')"]
          }
      }
  }

  def pre_spawn_hook(spawner):
      userhome_base = "/p/home/jusers"
      hub = spawner.extra_labels.get("app", "jupyter")
      stage = "staging"
      userid = str(spawner.user.id)
      profile = spawner.user_options.get("profile", "")
      try:
          service = profile.split("/")[0].lower()
      except:
          spawner.log.exception(f"{spawner._log_name} - Use default service: jupyterlab for skeleton dir")
          service = "jupyterlab"

      userhome_dir = os.path.join(userhome_base, hub, stage, userid)
      userhome_skel = f"{userhome_base}/skel/{service}"
      userhome_uid = 1000
      userhome_gid = 1000

      def copy_file(src, dst):
          shutil.copyfile(src=src, dst=dst)
          shutil.chown(dst, userhome_uid, userhome_gid)

      def copy_dir(src, dst):
          shutil.copytree(src=src, dst=dst)
          for dirpath, dirnames, filenames in os.walk(dst):
              shutil.chown(dirpath, userhome_uid, userhome_gid)
              for filename in filenames:
                  shutil.chown(os.path.join(dirpath, filename), userhome_uid, userhome_gid)

      if not os.path.exists(userhome_dir):
          spawner.log.info(f"{spawner._log_name} - Create home directory {userhome_dir}")
          os.makedirs(userhome_skel, exist_ok=True)
          copy_dir(userhome_skel, userhome_dir)
      else:
          # Update motd
          copy_file(f"{userhome_skel.rstrip('/')}/.motd", f"{userhome_dir.rstrip('/')}/.motd")
          shutil.rmtree(f"{userhome_dir.rstrip('/')}/.motd.d", ignore_errors=True)
          copy_dir(f"{userhome_skel.rstrip('/')}/.motd.d", f"{userhome_dir.rstrip('/')}/.motd.d")

  c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  def get_profile_volume_mounts(profile, flavor):
      common_mounts = [
          {
              "name": "nfs-software", 
              "mountPath": "/p/software/jsccloud", 
              "readOnly": True
          },
          {
              "name": "uftp", 
              "mountPath": "/tmp/custom/uftp.py",
              "subPath": "uftp.py"
          },
          {
              "name": "nfs-userhome",
              "mountPath": "/home/jovyan",
              "readOnly": False 
          }
      ]
      profile_mounts = [
        {
              "name": f"modules-{profile}", 
              "mountPath": "/tmp/custom/load_jupyter_version.sh", 
              "subPath": "load_jupyter_version.sh"
          },
          {
              "name": f"modules-user-{profile}", 
              "mountPath": "/tmp/custom/load_jupyter_modules.sh",
              "subPath": "load_jupyter_modules.sh"
          }
      ]
      flavor_mounts = [
          {
              "name": f"flavor-{flavor}",
              "mountPath": "/tmp/custom/load_flavor_variables.sh",
              "subPath": "load_flavor_variables.sh"
          }
      ]
      return common_mounts + profile_mounts + flavor_mounts
  

  def get_profile_volumes(profile, jupyterhub_name, flavor):
      stage = os.environ.get("STAGE", "staging")
      map_hub_names = {
          "jupyterjsc": "defaulthub",
          "coec": "defaulthub",
          "coeraise": "defaulthub",
          "portalgauss": "defaulthub"
      }
      hub = map_hub_names.get(jupyterhub_name, jupyterhub_name)
      common_volumes = [
          {
              "name": "nfs-software", 
              "hostPath": {
                  "path": "/p/software/jsccloud", 
                  "type": ""
              }
          },
          {
              "name": "uftp", 
              "configMap": {
                  "defaultMode": 292,
                  "name": "uftp"
              }
          },
          {
              "name": "nfs-userhome", 
              "hostPath": {
                  # userid gets expanded by KubeSpawner, do not use mapped 
                  # hub name here, each hub uses it's own home dir.
                  "path": f"/p/home/jusers/{jupyterhub_name}/{stage}/{{userid}}", 
                  "type": ""
              }
          }
      ]
      profile_volumes = [
          {
              "name": f"modules-{profile}", 
              "configMap": {
                  "defaultMode": 292,
                  "name": f"modules-{hub}-{profile}"
              }
          },
          {
              "name": f"modules-user-{profile}", 
              "configMap": {
                  "defaultMode": 292,
                  "name": f"modules-user-{hub}-{profile}"
              }
          }
      ]
      flavor_volumes = [
          {
              "name": f"flavor-{flavor}",
              "configMap": {
                  "defaultMode": 292,
                  "name": f"flavor-{flavor}"
              }
          }
      ]
      return common_volumes + profile_volumes + flavor_volumes
  
  # Guarantee is a bit less, to allow system pods run on the usernodes
  # as well
  all_flavor_resources = {
      "m1": {
          "cpu_guarantee": 0.5,
          "cpu_limit": 1,
          "mem_guarantee": "1536M",
          "mem_limit": "2048M"
      },
      "l1": {
          "cpu_guarantee": 0.5,
          "cpu_limit": 1,
          "mem_guarantee": "3584M",
          "mem_limit": "4096M"
      },
      "l2": {
          "cpu_guarantee": 0.5,
          "cpu_limit": 2,
          "mem_guarantee": "7680M",
          "mem_limit": "8192M"
      },
      "xl2": {
          "cpu_guarantee": 0.5,
          "cpu_limit": 2,
          "mem_guarantee": "15872M",
          "mem_limit": "16384M"
      }
  }

  def get_flavor_resources(flavor):
      if flavor not in all_flavor_resources.keys():
          raise Exception(f"Flavor {flavor} not configured. Abort start.")
      return all_flavor_resources["flavor"]

  async def profile_list(spawner):
      # We're loading this config file for each Spawner object
      # Therefore, it's fine to just create the one selected profile
      # with it's flavor 
      jupyterhub_name = spawner.jupyterhub_name
      flavor = spawner.user_options.get("flavor", "default")
      profile = spawner.user_options.get("profile", "JupyterLab/3.6")
      if "/" in profile:
          profile_split = '-'.join(profile.split("/")[1:])
      else:
          profile_split = profile
      mount_name = profile_split.replace(".", "-")

      resources = get_flavor_resources(flavor)

      return [
        {
            "display_name": "JupyterLab - 3.6",
            "slug": "JupyterLab/3.6",
            "kubespawner_override": {
                "volume_mounts": get_profile_volume_mounts(mount_name, flavor),
                "volumes": get_profile_volumes(mount_name, jupyterhub_name, flavor),
                "cpu_guarantee": resources["cpu_guarantee"],
                "cpu_limit": resources["cpu_limit"],
                "mem_guarantee": resources["mem_guarantee"],
                "mem_limit": resources["mem_limit"]
            }
        }
    ]

  c.KubeSpawner.profile_list = profile_list


