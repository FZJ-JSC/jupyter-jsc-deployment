apiVersion: v1
kind: ConfigMap
metadata:
  name: check-incidents
data:
  run.sh: |
    #!/bin/sh

    while [ ! -d /mnt/shared-data/share/jupyterhub/static/images/footer ]
    do 
        echo "Waiting for /mnt/shared-data/share/jupyterhub/static/images/footer ..."
        sleep 5
    done

    SLEEP=$(/usr/local/bin/python3 -c 'import yaml; f=open("/mnt/custom_config/jupyterhub_custom_config.yaml", "r"); config = yaml.full_load(f); f.close(); print(config.get("incidentCheck", {}).get("interval", 60))')
    while true
    do
        echo "$(date) - Run incident check ..."
        /usr/local/bin/python3 -u /mnt/incidents/check_incidents.py
        echo "$(date) - Run incident check ... done"
        sleep ${SLEEP}
    done

  check_incidents.py: |
    # Update incidents every n seconds
    import fcntl
    import json
    import logging.handlers
    import os
    import shutil
    import socket
    import sys
    import yaml

    import requests
    from dateutil import parser
    from jsonformatter import JsonFormatter

    static_dir = "/mnt/shared-data/share/jupyterhub/static/images/footer"
    incidents_json = "/mnt/shared-data/incidents/active_incidents.json"

    LOGGER_NAME = "IncidentCheck"
    log = logging.getLogger(LOGGER_NAME)
    log.level = 20

    supported_handler_classes = {
        "stream": logging.StreamHandler,
        "file": logging.handlers.TimedRotatingFileHandler,
        "smtp": logging.handlers.SMTPHandler,
        "syslog": logging.handlers.SysLogHandler,
    }

    # supported formatters and their arguments
    supported_formatter_classes = {"json": JsonFormatter, "simple": logging.Formatter}
    json_fmt = '{"asctime": "asctime", "levelno": "levelno", "levelname": "levelname", "logger": "name", "file": "pathname", "line": "lineno", "function": "funcName", "Message": "message"}'
    simple_fmt = "%(asctime)s logger=%(name)s levelno=%(levelno)s levelname=%(levelname)s file=%(pathname)s line=%(lineno)d function=%(funcName)s : %(message)s"
    supported_formatter_kwargs = {
        "json": {"fmt": json_fmt},
        "simple": {"fmt": simple_fmt},
    }


    def setup_logger(handler_name, configuration):
        configuration_logs = {"configuration": str(configuration)}
        formatter_name = configuration.pop("formatter")
        level = configuration.pop("level")

        # catch some special cases
        for key, value in configuration.items():
            if key == "stream":
                if value == "ext://sys.stdout":
                    configuration["stream"] = sys.stdout
                elif value == "ext://sys.stderr":
                    configuration["stream"] = sys.stderr
            elif key == "socktype":
                if value == "ext://socket.SOCK_STREAM":
                    configuration["socktype"] = socket.SOCK_STREAM
                elif value == "ext://socket.SOCK_DGRAM":
                    configuration["socktype"] = socket.SOCK_DGRAM
        handler = supported_handler_classes[handler_name](**configuration)
        formatter = supported_formatter_classes[formatter_name](
            **supported_formatter_kwargs[formatter_name]
        )
        handler.name = handler_name
        handler.setLevel(level)
        handler.setFormatter(formatter)
        logger = logging.getLogger(LOGGER_NAME)
        logger.addHandler(handler)
        log.debug(f"Logging handler added ({handler_name}): {configuration_logs}")


    def acquireReadLock(filename):
        """acquire exclusive lock file access"""
        if not os.path.exists(filename):
            with open(filename, "w") as f:
                f.write("")
        locked_file_descriptor = open(filename, "r+")
        fcntl.lockf(locked_file_descriptor, fcntl.LOCK_EX)
        return locked_file_descriptor


    def acquireLock(filename):
        """acquire exclusive lock file access"""
        if not os.path.exists(filename):
            with open(filename, "w") as f:
                f.write("")
        locked_file_descriptor = open(filename, "w+")
        fcntl.lockf(locked_file_descriptor, fcntl.LOCK_EX)
        return locked_file_descriptor


    def releaseLock(locked_file_descriptor):
        """release exclusive lock file access"""
        if locked_file_descriptor:
            locked_file_descriptor.close()


    def write_to_file(filename, fileinput):
        log.debug(f"Write to file {filename}: {fileinput}")
        try:
            lock_fd = acquireLock(filename) or None
            lock_fd.write(fileinput)
        except:
            log.exception(f"Could not write to file {filename}")
        finally:
            releaseLock(lock_fd)


    def read_json_file(filename):
        try:
            lock_fd = acquireReadLock(filename) or None
            with open(filename, "r") as f:
                ret = json.load(f)
        except:
            log.exception(f"Could not read from file {filename}")
            ret = None
        finally:
            releaseLock(lock_fd)
        return ret


    def update_thresholds(config):
        interactive_threshold = config.get("healthThreshold", {}).get("interactive", 40)
        compute_threshold = config.get("healthThreshold", {}).get("compute", 50)

        systems_incidents = read_json_file(incidents_json)
        systems_incidents["interactive_threshold"] = interactive_threshold
        systems_incidents["compute_threshold"] = compute_threshold
        write_to_file(incidents_json, json.dumps(systems_incidents, indent=2))
        

    def update_status_image(system, health):
        image_path = f"{static_dir}/systems/{system.lower()}.svg"
        # 0: Healthy, 10: Annotation, 20: Minor, 30: Medium, 40: Major, 50: Critical
        template_path = f"{static_dir}/templates/{health}.svg"
        try:
            log.info(f"Copy {template_path} to {image_path}")
            shutil.copyfile(template_path, image_path)
        except:
            log.exception(f"Could not copy {template_path} to {image_path}")


    def filter_and_sort_incidents(incidents_list):
        def _sort(incidents):
            incidents.sort(key=lambda x: x.get('incident_severity', 0), reverse=True)
            return incidents
        # FAIL > DEG > MAINT > ANNOT
        failures = [x for x in incidents_list if x.get('incident_type') == 'FAIL']
        if failures:
            return _sort(failures)
        degradations = [x for x in incidents_list if x.get('incident_type') == 'DEG']
        if degradations:
            return _sort(degradations)
        maintenances = [x for x in incidents_list if x.get('incident_type') == 'MAINT']
        if maintenances:
            return _sort(maintenances)
        # Do not return annotations as their short description is mostly unhelpful
        return []
        

    def get_info_msg(incidents_list):
        if len(incidents_list) > 1:
            log.warning(
                "Multiple active incidents of the same type. Use the highest severity one."
            )
        incident = incidents_list[0]
        short_description = incident["short_description"]
        if short_description:
            description = short_description
        else:
            description = incident["description"]
        start_time = incident["start_time"]
        if incident["end_time"]:
            end_time = incident["end_time"]
        else:
            end_time = "unknown"
        info_msg = f"{start_time} - {end_time}:\n{description}"
        return info_msg


    def update_incidents(system, svc, active_svc_incidents):
        systems_incidents = read_json_file(incidents_json)
        if not systems_incidents.get(system, {}):
            systems_incidents[system] = {}

        # Service has active incidents
        if active_svc_incidents:
            log.debug(f"Found active incidents for {system}.")
            systems_incidents[system]["incident"] = get_info_msg(active_svc_incidents)
        elif svc["next_maintenance"]:
            next_maintenance_incidents = [
                x
                for x in active_svc_incidents
                if parser.parse(x["start_time"]) == parser.parse(svc["next_maintenance"])
            ]
            if len(next_maintenance_incidents) == 0:
                raise Exception(f"Could not find matching start time in incidents for maintenance for {system}.")
            log.debug(f"Found announced maintenance(s) for {system}.")
            systems_incidents[system]["incident"] = get_info_msg(next_maintenance_incidents)
        else:
            systems_incidents[system]["incident"] = ""
        
        # Set initial status image if no health status exists yet
        if "health" not in systems_incidents.get(system):
            update_status_image(system, svc["health"])
        # Change status image if service has a new health status
        elif svc["health"] != systems_incidents.get(system).get("health", 0):
            update_status_image(system, svc["health"])
        systems_incidents.get(system)["health"] = svc["health"]

        write_to_file(incidents_json, json.dumps(systems_incidents, indent=2))


    def check_incidents(config):
        update_thresholds(config)

        api_url = config.get("url", "https://status.jsc.fz-juelich.de/api")
        try:
            all_incidents_r = requests.get(f"{api_url}/incidents", timeout=5)
            all_incidents_r.raise_for_status()
            all_incidents = all_incidents_r.json()
            for name, id in config["services"].items():
                try:
                    svc_r = requests.get(f"{api_url}/services/{id}", timeout=5)
                    svc_r.raise_for_status()
                    svc = svc_r.json()
                    active_svc_incidents = [
                        x for x in all_incidents 
                            if int(id) in x.get("affected_services", []) 
                            and not x.get("resolved", "")
                    ]
                    active_svc_incidents = filter_and_sort_incidents(active_svc_incidents)
                    update_incidents(name, svc, active_svc_incidents)
                except:
                    log.exception(f"Could not check for incidents for {name}")
        except:
            log.exception("Could not check for incidents")


    if __name__ == "__main__":
        with open("/mnt/custom_config/jupyterhub_custom_config.yaml", "r") as f:
            config_all = yaml.full_load(f)
        config = config_all.get("incidentCheck", {})
        for name, logger_config in config.get("logger", {}).items():
            setup_logger(name, logger_config)
        
        if not os.path.exists(incidents_json):
            with open(incidents_json, "w") as f:
                f.write(json.dumps({}))
        if not os.path.isfile(incidents_json):
            log.error(f"{incidents_json} must be a file. Exit")
            exit(1)

        check_incidents(config)

